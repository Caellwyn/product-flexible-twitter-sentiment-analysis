{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pathlib\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "import re\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Function takes in a string and assigns it a part of speech tag.\n",
    "    Used for lemmatizing, no need to use elsewhere\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def product_target(string):\n",
    "    '''\n",
    "    Description:\n",
    "    - Method used for finding a string that the clean function will replace with 'product_target' if it sees any instances of it.\n",
    "    \n",
    "    Inputs:\n",
    "    - Takes in the data from the product column through the string parameter.\n",
    "    \n",
    "    Steps:\n",
    "    1. sets the target variable to lower case\n",
    "    2. searches what the variable is and return a preset value that we will replace with 'product_target'\n",
    "    \n",
    "    Outputs:\n",
    "    - Returns a string\n",
    "    '''\n",
    "    s = string.lower()\n",
    "    if s == 'no target':\n",
    "        return ''\n",
    "    elif s == 'ipad':\n",
    "        return 'ipad'\n",
    "    elif s == 'apple':\n",
    "        return 'apple'\n",
    "    elif s == 'ipad or iphone app':\n",
    "        return 'app'\n",
    "    elif s == 'iphone':\n",
    "        return 'iphone'\n",
    "    elif s == 'other apple product or service':\n",
    "        return ''\n",
    "    elif s == 'google':\n",
    "        return 'google'\n",
    "    elif s == 'other google product or service':\n",
    "        return ''\n",
    "    elif s == 'android':\n",
    "        return 'android'\n",
    "    elif s == 'android app':\n",
    "        return 'android'\n",
    "    else:\n",
    "        return 'Unknown target'\n",
    "\n",
    "def txt_clean(txt, lem):\n",
    "    '''\n",
    "    Description\n",
    "    - A method we use to clean every inputed string and prepare it for model processing\n",
    "    \n",
    "    Inputs:\n",
    "    - Takes in a string and returns a cleaned up version of it.\n",
    "    - Takes in a  boolean variable to determine whether or not the lemmatizing function will be used.\n",
    "    \n",
    "    Steps:\n",
    "    1. Split the tweet into tokens\n",
    "    2. Convert all capitalized letters into lower-case\n",
    "    3. Remove punctuation\n",
    "    4. Remove twitter jargon such as @ mentions\n",
    "    5. Remove leftover numbers\n",
    "    6. Remove words with accents\n",
    "    7. Remove stop words\n",
    "    8. Replace instances of the target in the text with 'product_target'\n",
    "    9. Remove empty strings\n",
    "    10. Lemmatize the words\n",
    "    11. Rejoin all the tokens into one string\n",
    "    \n",
    "    Outputs:\n",
    "    - A cleaned up string of words ready for model processing.\n",
    "    '''\n",
    "    sw = stopwords.words('english')\n",
    "    sw.extend(['link', 'rt', 'get'])\n",
    "    punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~“!#'\n",
    "    no_accents_re = re.compile('^[a-z]+$')\n",
    "    accents = ['á', 'â', 'ã', 'à', 'å', 'ª', 'ç', 'è', '¼', '¾', 'î', 'ï', 'ì', 'ó', 'ö', 'ð', 'ü', 'ù', 'û', 'ý']\n",
    "    twitter_re = re.compile('[@][a-zA-Z]*')\n",
    "    num_re = re.compile('^\\d+$')\n",
    "    \n",
    "    # splitting the text up into words\n",
    "    if isinstance(txt, list):\n",
    "        t = txt[0].split(' ')\n",
    "    else:\n",
    "        t = txt.split(' ')\n",
    "    # turning the words lowercase\n",
    "    t = [w.lower() for w in t]\n",
    "    # removing punctuation\n",
    "    t = [w.translate(w.maketrans('','', punctuation)) for w in t]\n",
    "    # removing @'s which are twitter jargon\n",
    "    t = [w for w in t if not twitter_re.match(w)]\n",
    "    # removing leftover numbers\n",
    "    t = [w for w in t if not num_re.match(w)]\n",
    "    # removing words with accents\n",
    "    t = [w for w in t if no_accents_re.match(w)]\n",
    "    # removing stop words and more twitter jargon\n",
    "    t = [w for w in t if w not in sw]\n",
    "    # change targets in string to 'product_target' if a target exists\n",
    "    if isinstance(txt, list):\n",
    "        t = ['product_target' if w in product_target(txt[1]) else w for w in t]\n",
    "        if txt[1].lower() in ['android app', 'ipad or iphone app']:\n",
    "            t = [w for w in t if w is not 'app']\n",
    "    # removing empty strings\n",
    "    t = [w for w in t if w]\n",
    "    # word lemmatizing\n",
    "    if lem: \n",
    "        lemm = WordNetLemmatizer()\n",
    "        t = pos_tag(t)\n",
    "        t = [(w[0], get_wordnet_pos(w[1])) for w in t]\n",
    "        t = [lemm.lemmatize(w[0], w[1]) for w in t]\n",
    "    # joining all the strings together into one\n",
    "    return ' '.join(t)\n",
    "\n",
    "def emotion_label(string):\n",
    "    '''\n",
    "    Description:\n",
    "    - Simple mapping function used to turn our targets from words into a numerical value\n",
    "    \n",
    "    Inputs:\n",
    "    - A string\n",
    "    \n",
    "    Steps:\n",
    "    1. Matches the input string with a preset value, positive is a 2, negative is a 0, and neutral is a 1\n",
    "    \n",
    "    Output:\n",
    "    - The corresponding integer value\n",
    "    '''\n",
    "    s = string\n",
    "    if s == 'Positive emotion':\n",
    "        return 2\n",
    "    elif s == 'No emotion toward brand or product':\n",
    "        return 1\n",
    "    elif s == 'Negative emotion':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unknown emotion')\n",
    "\n",
    "def df_clean(df = None, lem = True):\n",
    "    '''\n",
    "    Description:\n",
    "    - A function that returns a cleaned up dataframe.\n",
    "    \n",
    "    Inputs:\n",
    "    - It can take a dataframe, if no dataframe is passed through it will use our default one.\n",
    "    - It can also set a boolean value, it is used as a flag to determine if lemmatizing will be used.\n",
    "    \n",
    "    Steps:\n",
    "    1. A dataframe will be either created or brought in from an external source determined by the parameter\n",
    "    2. The dataframe will then be adjusted to make reading easier\n",
    "    3. NaN values and values we don't want will be removed\n",
    "    4. We will use the emotion_label to change our emotion values into numerical\n",
    "    5. We will use the txt_clean function on the tweet text for every row \n",
    "    6. Lastly we will drop unused columns\n",
    "    \n",
    "    Outputs:\n",
    "    - A cleaned dataframe that we will use for our models\n",
    "    '''\n",
    "    if df is None:\n",
    "        df = pd.read_csv('../../data/judge-1377884607_tweet_product_company.csv', encoding = 'latin1')\n",
    "    df.columns = ['text', 'product', 'emotion']\n",
    "    df = df[df['emotion'] != 'I can\\'t tell']\n",
    "    df.dropna(inplace = True)\n",
    "    df['text_product'] = df.apply(lambda x: list([x['text'], x['product']]), axis = 1)\n",
    "    df['emotion'] = df['emotion'].map(emotion_label)\n",
    "    df['txt_cleaned'] = df['text_product'].apply(txt_clean, args = (lem,))\n",
    "    df.drop(columns = ['text', 'product', 'text_product'], inplace = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def external_data(lem = True):\n",
    "    '''\n",
    "    Description:\n",
    "    - A function that gives us the cleaned up dataframes of our three external datasets\n",
    "    - It will be cleaned in a way similar to our base dataset\n",
    "    \n",
    "    Inputs:\n",
    "    - A boolean variable that will determine whether or not a lemmatizer is used or not\n",
    "    \n",
    "    Steps:\n",
    "    1. Reads in the data from our datasets to create three dfs\n",
    "    2. For each df, grab the columns that we care about and rename them to match our base data\n",
    "    3. We then map our emotions column in a way that matches our base data (0, 1, and 2 variables)\n",
    "    4. Use our txt_clean function to clean up the texts in the dataset\n",
    "    5. Drop all unused columns\n",
    "    6. Concat all three together\n",
    "    \n",
    "    Outputs:\n",
    "    - A big dataframe containing cleaned up version of all the tweets from external sources\n",
    "    - Ready to be added directly to the end of our base data\n",
    "    '''\n",
    "\n",
    "    df_1 = pd.read_csv('../../data/Apple-Twitter-Sentiment-DFE.csv', encoding = 'latin1')\n",
    "    df_1 = df_1[['sentiment', 'text']]\n",
    "    df_1.columns = ['emotion', 'text']\n",
    "    dic_1 = {'5': 2, '3' : 1, '1': 0}\n",
    "    df_1.replace({'emotion': dic_1}, inplace = True)\n",
    "    df_1['txt_cleaned'] = df_1['text'].apply(txt_clean, args = (lem,))\n",
    "    df_1.drop('text', axis = 1, inplace = True)\n",
    "\n",
    "    df_2 = pd.read_csv('../../data/Deflategate-DFE.csv', encoding = 'latin1')\n",
    "    df_2 = df_2[['deflate_sentiment', 'text']]\n",
    "    df_2.columns = ['emotion', 'text']\n",
    "    dic_2 = {'positive': 2, 'slightly positive': 2, ('neutral') : 1, 'negative': 0, 'slightly negative': 0}\n",
    "    df_2.replace({'emotion': dic_2}, inplace = True)\n",
    "    df_2['txt_cleaned'] = df_2['text'].apply(txt_clean, args = (lem,))\n",
    "    df_2.drop('text', axis = 1, inplace = True)\n",
    "\n",
    "    df_3 = pd.read_csv('../../data/Coachella-2015-2-DFE.csv', encoding = 'latin1')\n",
    "    df_3 = df_3[['coachella_sentiment', 'text']]\n",
    "    df_3.columns = ['emotion', 'text']\n",
    "    df_3 = df_3[df_3['emotion'] != 'cant tell']\n",
    "    dic_3 = {'positive': 2, 'neutral' : 1, 'negative': 0}\n",
    "    df_3.replace({'emotion': dic_3}, inplace = True)\n",
    "    df_3['txt_cleaned'] = df_3['text'].apply(txt_clean, args = (lem,))\n",
    "    df_3.drop('text', axis = 1, inplace = True)\n",
    "\n",
    "    return pd.concat([df_1, df_2, df_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef = external_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, ef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22746, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divides data into X and y, and then turns the model target labels into numerical format\n",
    "\n",
    "X = df['txt_cleaned']\n",
    "y = df['emotion'].replace(to_replace = {'Positive emotion' : 0, 'Negative emotion' : 1, 'No emotion toward brand or product': 2})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.25)\n",
    "X_t, X_val, y_t, y_val = train_test_split(X, y, random_state = 42, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    def __init__(self, vec_type, ngram = (1,1)):\n",
    "        if type(ngram) is not tuple:\n",
    "            print('Unknown tuple, format should be (minimum n-gram, maximum n-gram)')\n",
    "            return False\n",
    "        \n",
    "        if vec_type == 'cv':\n",
    "            self.vec = CountVectorizer(ngram_range = ngram)\n",
    "        elif vec_type == 'tfidf':\n",
    "            self.vec = TfidfVectorizer(ngram_range = ngram)\n",
    "        else:\n",
    "            print('Unknown vectorizer type')\n",
    "            return False\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        self.vec.fit(X)\n",
    "\n",
    "    def transform(self, X, y):\n",
    "        X_vec = self.vec.transform(X)\n",
    "        X_vec = pd.DataFrame.sparse.from_spmatrix(X_vec)\n",
    "        X_vec.columns = sorted(self.vec.vocabulary_)\n",
    "        X_vec.set_index(y.index, inplace = True)\n",
    "        return X_vec\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        self.vec.fit(X)\n",
    "        X_vec = self.vec.transform(X)\n",
    "        X_vec = pd.DataFrame.sparse.from_spmatrix(X_vec)\n",
    "        X_vec.columns = sorted(self.vec.vocabulary_)\n",
    "        X_vec.set_index(y.index, inplace = True)\n",
    "        return X_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "store      1113\n",
       "new         802\n",
       "austin      688\n",
       "launch      620\n",
       "app         598\n",
       "amp         550\n",
       "social      481\n",
       "circle      468\n",
       "popup       427\n",
       "today       422\n",
       "android     419\n",
       "open        359\n",
       "network     355\n",
       "go          350\n",
       "line        335\n",
       "via         321\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range = (1,1))\n",
    "X_t_vec = cv.fit_transform(X_t)\n",
    "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(cv.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "X_val_vec = cv.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(cv.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "X_t_vec.sum(axis = 0).sort_values(ascending = False)[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "social network     338\n",
       "popup store        325\n",
       "new social         308\n",
       "network call       239\n",
       "call circle        222\n",
       "major new          220\n",
       "launch major       213\n",
       "temporary store    187\n",
       "possibly today     179\n",
       "circle possibly    170\n",
       "downtown austin    138\n",
       "ûï mention         134\n",
       "marissa mayer      132\n",
       "store downtown     132\n",
       "store austin       130\n",
       "open popup         121\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range = (2,2))\n",
    "X_t_vec = cv.fit_transform(X_t)\n",
    "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(cv.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "X_val_vec = cv.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(cv.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "X_t_vec.sum(axis = 0).sort_values(ascending = False)[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new social network          288\n",
       "social network call         238\n",
       "major new social            219\n",
       "network call circle         215\n",
       "launch major new            213\n",
       "call circle possibly        168\n",
       "circle possibly today       168\n",
       "store downtown austin       117\n",
       "open temporary store        108\n",
       "temporary store downtown     77\n",
       "popup store austin           64\n",
       "open popup store             63\n",
       "open popup shop              56\n",
       "downtown austin launch       55\n",
       "launch new social            49\n",
       "rumor open temporary         48\n",
       "dtype: int64"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range = (3,3))\n",
    "X_t_vec = cv.fit_transform(X_t)\n",
    "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(cv.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "X_val_vec = cv.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(cv.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "X_t_vec.sum(axis = 0).sort_values(ascending = False)[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "store      232.754934\n",
       "new        164.683823\n",
       "launch     151.630980\n",
       "austin     139.152442\n",
       "app        124.349935\n",
       "social     121.266819\n",
       "popup      117.843415\n",
       "circle     114.139122\n",
       "today      107.380222\n",
       "open       107.017833\n",
       "amp        104.570787\n",
       "network    103.692296\n",
       "via         90.282673\n",
       "line        87.238168\n",
       "call        85.940698\n",
       "go          80.454228\n",
       "dtype: float64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range = (1,1))\n",
    "X_t_vec = tfidf.fit_transform(X_t)\n",
    "X_t_vec = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace = True)\n",
    "\n",
    "X_val_vec = tfidf.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "X_t_vec.sum(axis = 0).sort_values(ascending = False)[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "social network     0.012871\n",
       "new social         0.012075\n",
       "popup store        0.011281\n",
       "network call       0.010188\n",
       "call circle        0.009623\n",
       "major new          0.009447\n",
       "launch major       0.009276\n",
       "possibly today     0.008182\n",
       "temporary store    0.008133\n",
       "circle possibly    0.007913\n",
       "store austin       0.006158\n",
       "downtown austin    0.005948\n",
       "open popup         0.005938\n",
       "store downtown     0.005905\n",
       "open temporary     0.005599\n",
       "ûï mention         0.004758\n",
       "dtype: float64"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range = (2,2))\n",
    "X_t_vec = tfidf.fit_transform(X_t)\n",
    "X_t_vec = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace = True)\n",
    "\n",
    "X_val_vec = tfidf.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "X_t_vec.mean(axis = 0).sort_values(ascending = False)[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new social network          80.137694\n",
       "social network call         69.935329\n",
       "network call circle         65.313997\n",
       "major new social            64.712864\n",
       "launch major new            63.743781\n",
       "call circle possibly        54.022252\n",
       "circle possibly today       54.005559\n",
       "open temporary store        38.100215\n",
       "store downtown austin       36.312710\n",
       "temporary store downtown    27.969892\n",
       "popup store austin          24.687708\n",
       "open popup store            24.318210\n",
       "downtown austin launch      23.002409\n",
       "open popup shop             21.364177\n",
       "launch new social           20.361676\n",
       "rumor open temporary        19.746143\n",
       "dtype: float64"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range = (3,3))\n",
    "X_t_vec = tfidf.fit_transform(X_t)\n",
    "X_t_vec = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace = True)\n",
    "\n",
    "X_val_vec = tfidf.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "X_t_vec.sum(axis = 0).sort_values(ascending = False)[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aapl</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abacus</th>\n",
       "      <th>abba</th>\n",
       "      <th>aber</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abnormal</th>\n",
       "      <th>abound</th>\n",
       "      <th>...</th>\n",
       "      <th>zlf</th>\n",
       "      <th>zms</th>\n",
       "      <th>zomb</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zomg</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zynga</th>\n",
       "      <th>zzzs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3204</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4460</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6298</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5837</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5285</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5488</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7396</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6702 rows × 7507 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aapl  aaron  ab  abacus  abba  aber  ability  able  abnormal  abound  \\\n",
       "3363     0      0   0       0     0     0        0     0         0       0   \n",
       "3204     0      0   0       0     0     0        0     0         0       0   \n",
       "4460     0      0   0       0     0     0        0     0         0       0   \n",
       "2311     0      0   0       0     0     0        0     0         0       0   \n",
       "6298     0      0   0       0     0     0        0     0         0       0   \n",
       "...    ...    ...  ..     ...   ...   ...      ...   ...       ...     ...   \n",
       "5837     0      0   0       0     0     0        0     0         0       0   \n",
       "5285     0      0   0       0     0     0        0     0         0       0   \n",
       "5488     0      0   0       0     0     0        0     0         0       0   \n",
       "873      0      0   0       0     0     0        0     0         0       0   \n",
       "7396     0      0   0       0     0     0        0     0         0       0   \n",
       "\n",
       "      ...  zlf  zms  zomb  zombie  zomg  zone  zoom  zuckerberg  zynga  zzzs  \n",
       "3363  ...    0    0     0       0     0     0     0           0      0     0  \n",
       "3204  ...    0    0     0       0     0     0     0           0      0     0  \n",
       "4460  ...    0    0     0       0     0     0     0           0      0     0  \n",
       "2311  ...    0    0     0       0     0     0     0           0      0     0  \n",
       "6298  ...    0    0     0       0     0     0     0           0      0     0  \n",
       "...   ...  ...  ...   ...     ...   ...   ...   ...         ...    ...   ...  \n",
       "5837  ...    0    0     0       0     0     0     0           0      0     0  \n",
       "5285  ...    0    0     0       0     0     0     0           0      0     0  \n",
       "5488  ...    0    0     0       0     0     0     0           0      0     0  \n",
       "873   ...    0    0     0       0     0     0     0           0      0     0  \n",
       "7396  ...    0    0     0       0     0     0     0           0      0     0  \n",
       "\n",
       "[6702 rows x 7507 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count or tfidf\n",
    "# n-gram range\n",
    "# data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
